{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e56657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Training Loss: 2.372253288068793  Time (sec): 0.75260329246521\n",
      "Epoch: 2  Training Loss: 2.0845882980288866  Time (sec): 0.7641246318817139\n",
      "Epoch: 3  Training Loss: 1.9713873608305685  Time (sec): 0.7102222442626953\n",
      "Epoch: 4  Training Loss: 1.8999134299858793  Time (sec): 0.7406079769134521\n",
      "Epoch: 5  Training Loss: 1.849317634692144  Time (sec): 0.7371089458465576\n",
      "Epoch: 6  Training Loss: 1.8131982568550509  Time (sec): 0.7433063983917236\n",
      "Epoch: 7  Training Loss: 1.7782646196933243  Time (sec): 0.7501199245452881\n",
      "Epoch: 8  Training Loss: 1.750280506110108  Time (sec): 0.7205286026000977\n",
      "Epoch: 9  Training Loss: 1.7241403931127752  Time (sec): 0.711613655090332\n",
      "Epoch: 10  Training Loss: 1.7014885660128845  Time (sec): 0.7366127967834473\n",
      "Epoch: 11  Training Loss: 1.6782430977974552  Time (sec): 0.7351441383361816\n",
      "Epoch: 12  Training Loss: 1.658977648306035  Time (sec): 0.7221775054931641\n",
      "Epoch: 13  Training Loss: 1.6372937916786052  Time (sec): 0.7336030006408691\n",
      "Epoch: 14  Training Loss: 1.6202109893297036  Time (sec): 0.7306263446807861\n",
      "Epoch: 15  Training Loss: 1.601390976173066  Time (sec): 0.738623857498169\n",
      "Epoch: 16  Training Loss: 1.5870563174186387  Time (sec): 0.7346153259277344\n",
      "Epoch: 17  Training Loss: 1.5726983428954688  Time (sec): 0.763674259185791\n",
      "Epoch: 18  Training Loss: 1.5587959323966052  Time (sec): 0.7547030448913574\n",
      "Epoch: 19  Training Loss: 1.5470856363873573  Time (sec): 0.7226347923278809\n",
      "Epoch: 20  Training Loss: 1.5370586452621497  Time (sec): 0.7566182613372803\n",
      "Epoch: 21  Training Loss: 1.5276840507689653  Time (sec): 0.742396354675293\n",
      "Epoch: 22  Training Loss: 1.5221633897067768  Time (sec): 0.7380414009094238\n",
      "Epoch: 23  Training Loss: 1.5150726585729406  Time (sec): 0.7588250637054443\n",
      "Epoch: 24  Training Loss: 1.5074396718586305  Time (sec): 0.7436232566833496\n",
      "Epoch: 25  Training Loss: 1.5018118948540564  Time (sec): 0.7971258163452148\n",
      "Epoch: 26  Training Loss: 1.497411846272332  Time (sec): 0.7867426872253418\n",
      "Epoch: 27  Training Loss: 1.490899997164512  Time (sec): 0.7801473140716553\n",
      "Epoch: 28  Training Loss: 1.4873080117678983  Time (sec): 0.774132251739502\n",
      "Epoch: 29  Training Loss: 1.4843351159323785  Time (sec): 0.7606086730957031\n",
      "Epoch: 30  Training Loss: 1.481016887462117  Time (sec): 0.7631411552429199\n",
      "Epoch: 31  Training Loss: 1.4811421354698624  Time (sec): 0.7536048889160156\n",
      "Epoch: 32  Training Loss: 1.4726471722441552  Time (sec): 0.7440600395202637\n",
      "Epoch: 33  Training Loss: 1.4738406342302042  Time (sec): 0.7746469974517822\n",
      "Epoch: 34  Training Loss: 1.4726005469324335  Time (sec): 0.715587854385376\n",
      "Epoch: 35  Training Loss: 1.4696659554988685  Time (sec): 0.7526168823242188\n",
      "Epoch: 36  Training Loss: 1.4642083983466114  Time (sec): 0.7401199340820312\n",
      "Epoch: 37  Training Loss: 1.4631292406128156  Time (sec): 0.773608922958374\n",
      "Epoch: 38  Training Loss: 1.4613576104534562  Time (sec): 0.7251167297363281\n",
      "Epoch: 39  Training Loss: 1.461860547212006  Time (sec): 0.7572236061096191\n",
      "Epoch: 40  Training Loss: 1.4591551712434365  Time (sec): 0.7517035007476807\n",
      "Epoch: 41  Training Loss: 1.4533532332749697  Time (sec): 0.7346012592315674\n",
      "Epoch: 42  Training Loss: 1.4526140836893533  Time (sec): 0.7266139984130859\n",
      "Epoch: 43  Training Loss: 1.4575994715705713  Time (sec): 0.7181203365325928\n",
      "Epoch: 44  Training Loss: 1.4531656837992861  Time (sec): 0.7456183433532715\n",
      "Epoch: 45  Training Loss: 1.451068638274437  Time (sec): 0.7395961284637451\n",
      "Epoch: 46  Training Loss: 1.44934174648743  Time (sec): 0.7262535095214844\n",
      "Epoch: 47  Training Loss: 1.4488523637620594  Time (sec): 0.7452657222747803\n",
      "Epoch: 48  Training Loss: 1.4473748846591747  Time (sec): 0.7266089916229248\n",
      "Epoch: 49  Training Loss: 1.4467561104743367  Time (sec): 0.7296872138977051\n",
      "Epoch: 50  Training Loss: 1.4444360574829367  Time (sec): 0.7596175670623779\n",
      "Epoch: 1  Training Loss: 2.2195624570115497  Time (sec): 1.4724390506744385\n",
      "Epoch: 2  Training Loss: 1.9551474323062443  Time (sec): 1.3732457160949707\n",
      "Epoch: 3  Training Loss: 1.8653364349900785  Time (sec): 1.3636062145233154\n",
      "Epoch: 4  Training Loss: 1.80814631593199  Time (sec): 1.4060132503509521\n",
      "Epoch: 5  Training Loss: 1.7600446800454432  Time (sec): 1.4126341342926025\n",
      "Epoch: 6  Training Loss: 1.7196859966486377  Time (sec): 1.5862245559692383\n",
      "Epoch: 7  Training Loss: 1.6869824360196164  Time (sec): 1.454423427581787\n",
      "Epoch: 8  Training Loss: 1.6550898045174207  Time (sec): 1.40126371383667\n",
      "Epoch: 9  Training Loss: 1.6281416013461572  Time (sec): 1.3871867656707764\n",
      "Epoch: 10  Training Loss: 1.6042456104938236  Time (sec): 1.4018378257751465\n",
      "Epoch: 11  Training Loss: 1.5803708383691868  Time (sec): 1.3812623023986816\n",
      "Epoch: 12  Training Loss: 1.5588880561238263  Time (sec): 1.4102485179901123\n",
      "Epoch: 13  Training Loss: 1.540041888860886  Time (sec): 1.3932254314422607\n",
      "Epoch: 14  Training Loss: 1.5237709301624975  Time (sec): 1.4067859649658203\n",
      "Epoch: 15  Training Loss: 1.507851448261732  Time (sec): 1.368368148803711\n",
      "Epoch: 16  Training Loss: 1.499386053023811  Time (sec): 1.3982741832733154\n",
      "Epoch: 17  Training Loss: 1.489959122412129  Time (sec): 1.399240493774414\n",
      "Epoch: 18  Training Loss: 1.4837255980062953  Time (sec): 1.3932793140411377\n",
      "Epoch: 19  Training Loss: 1.4794195424414873  Time (sec): 1.4082136154174805\n",
      "Epoch: 20  Training Loss: 1.4722276153663214  Time (sec): 1.4094295501708984\n",
      "Epoch: 21  Training Loss: 1.4661555125565158  Time (sec): 1.4534032344818115\n",
      "Epoch: 22  Training Loss: 1.4608236907688217  Time (sec): 1.5050337314605713\n",
      "Epoch: 23  Training Loss: 1.4482527480560607  Time (sec): 1.4447681903839111\n",
      "Epoch: 24  Training Loss: 1.4450679636693118  Time (sec): 1.4953105449676514\n",
      "Epoch: 25  Training Loss: 1.4423799326199827  Time (sec): 1.4526522159576416\n",
      "Epoch: 26  Training Loss: 1.4311461940012  Time (sec): 1.466447114944458\n",
      "Epoch: 27  Training Loss: 1.4279651344381423  Time (sec): 1.3641676902770996\n",
      "Epoch: 28  Training Loss: 1.413193900642325  Time (sec): 1.4666147232055664\n",
      "Epoch: 29  Training Loss: 1.4083593642588155  Time (sec): 1.4434797763824463\n",
      "Epoch: 30  Training Loss: 1.400148761830551  Time (sec): 1.4453439712524414\n",
      "Epoch: 31  Training Loss: 1.3940135658922765  Time (sec): 1.44447660446167\n",
      "Epoch: 32  Training Loss: 1.3899504259335473  Time (sec): 1.4549663066864014\n",
      "Epoch: 33  Training Loss: 1.3757291331512076  Time (sec): 1.4638760089874268\n",
      "Epoch: 34  Training Loss: 1.3714405590699736  Time (sec): 1.49979829788208\n",
      "Epoch: 35  Training Loss: 1.3677954168982955  Time (sec): 1.4762051105499268\n",
      "Epoch: 36  Training Loss: 1.360630861254602  Time (sec): 1.477475643157959\n",
      "Epoch: 37  Training Loss: 1.3560524642924823  Time (sec): 1.3714783191680908\n",
      "Epoch: 38  Training Loss: 1.3512960415799  Time (sec): 1.445448398590088\n",
      "Epoch: 39  Training Loss: 1.3455447119810877  Time (sec): 1.3762588500976562\n",
      "Epoch: 40  Training Loss: 1.334573729106319  Time (sec): 1.3892815113067627\n",
      "Epoch: 41  Training Loss: 1.331515570048466  Time (sec): 1.4042880535125732\n",
      "Epoch: 42  Training Loss: 1.3341046815655164  Time (sec): 1.3822228908538818\n",
      "Epoch: 43  Training Loss: 1.3236456956369174  Time (sec): 1.3296983242034912\n",
      "Epoch: 44  Training Loss: 1.3188312725076115  Time (sec): 1.351045846939087\n",
      "Epoch: 45  Training Loss: 1.3189228399800672  Time (sec): 1.3636960983276367\n",
      "Epoch: 46  Training Loss: 1.3107652566916528  Time (sec): 1.4030177593231201\n",
      "Epoch: 47  Training Loss: 1.3065836546355594  Time (sec): 1.4467711448669434\n",
      "Epoch: 48  Training Loss: 1.3105011438295135  Time (sec): 1.4709830284118652\n",
      "Epoch: 49  Training Loss: 1.303025116707736  Time (sec): 1.3593125343322754\n",
      "Epoch: 50  Training Loss: 1.3075697934198283  Time (sec): 1.359236478805542\n",
      "Epoch: 1  Training Loss: 2.219754643508514  Time (sec): 4.452636957168579\n",
      "Epoch: 2  Training Loss: 1.951666919253358  Time (sec): 4.232882738113403\n",
      "Epoch: 3  Training Loss: 1.8426453987571907  Time (sec): 4.08594822883606\n",
      "Epoch: 4  Training Loss: 1.7664598071972508  Time (sec): 4.109316825866699\n",
      "Epoch: 5  Training Loss: 1.70473327966669  Time (sec): 4.170733451843262\n",
      "Epoch: 6  Training Loss: 1.6525171193590218  Time (sec): 4.026782751083374\n",
      "Epoch: 7  Training Loss: 1.607729474673056  Time (sec): 3.795557975769043\n",
      "Epoch: 8  Training Loss: 1.5676415281154077  Time (sec): 3.835704803466797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Training Loss: 1.5311751606209285  Time (sec): 3.902515411376953\n",
      "Epoch: 10  Training Loss: 1.4983155740032539  Time (sec): 3.7428245544433594\n",
      "Epoch: 11  Training Loss: 1.4708199706516947  Time (sec): 3.799710988998413\n",
      "Epoch: 12  Training Loss: 1.4431091799361984  Time (sec): 3.6963391304016113\n",
      "Epoch: 13  Training Loss: 1.416872011493659  Time (sec): 3.7900736331939697\n",
      "Epoch: 14  Training Loss: 1.4007046613408296  Time (sec): 3.7288930416107178\n",
      "Epoch: 15  Training Loss: 1.3880186303490425  Time (sec): 3.7557945251464844\n",
      "Epoch: 16  Training Loss: 1.3814216374945716  Time (sec): 3.742213726043701\n",
      "Epoch: 17  Training Loss: 1.374415042614055  Time (sec): 3.788393259048462\n",
      "Epoch: 18  Training Loss: 1.3752585219150257  Time (sec): 3.688690185546875\n",
      "Epoch: 19  Training Loss: 1.374034144905511  Time (sec): 3.7460196018218994\n",
      "Epoch: 20  Training Loss: 1.369412892048244  Time (sec): 3.8089945316314697\n",
      "Epoch: 21  Training Loss: 1.3675253540408703  Time (sec): 3.77437686920166\n",
      "Epoch: 22  Training Loss: 1.3692810724025772  Time (sec): 3.7805705070495605\n",
      "Epoch: 23  Training Loss: 1.367992037837209  Time (sec): 3.687570095062256\n",
      "Epoch: 24  Training Loss: 1.3575106595984976  Time (sec): 3.887192964553833\n",
      "Epoch: 25  Training Loss: 1.3613958822946748  Time (sec): 4.2759668827056885\n",
      "Epoch: 26  Training Loss: 1.3542371893448595  Time (sec): 4.0492682456970215\n",
      "Epoch: 27  Training Loss: 1.356241046973804  Time (sec): 3.696629762649536\n",
      "Epoch: 28  Training Loss: 1.35108441807273  Time (sec): 3.7406747341156006\n",
      "Epoch: 29  Training Loss: 1.3463874408038665  Time (sec): 3.7907662391662598\n",
      "Epoch: 30  Training Loss: 1.3428998374382657  Time (sec): 3.9091553688049316\n",
      "Epoch: 31  Training Loss: 1.3402525581237101  Time (sec): 3.851214647293091\n",
      "Epoch: 32  Training Loss: 1.3350519311850952  Time (sec): 3.778693675994873\n",
      "Epoch: 33  Training Loss: 1.3310594832093914  Time (sec): 3.76631498336792\n",
      "Epoch: 34  Training Loss: 1.3220214015946985  Time (sec): 3.680323362350464\n",
      "Epoch: 35  Training Loss: 1.3208855743011436  Time (sec): 3.742310047149658\n",
      "Epoch: 36  Training Loss: 1.3172356334856903  Time (sec): 3.7107017040252686\n",
      "Epoch: 37  Training Loss: 1.3129749670461914  Time (sec): 3.8330090045928955\n",
      "Epoch: 38  Training Loss: 1.3109437274693192  Time (sec): 3.6604387760162354\n",
      "Epoch: 39  Training Loss: 1.3035663974125415  Time (sec): 3.7125473022460938\n",
      "Epoch: 40  Training Loss: 1.3030493515801047  Time (sec): 3.754190444946289\n",
      "Epoch: 41  Training Loss: 1.2999198899254527  Time (sec): 3.750558614730835\n",
      "Epoch: 42  Training Loss: 1.3017177024364632  Time (sec): 3.721320390701294\n",
      "Epoch: 43  Training Loss: 1.2947999039402747  Time (sec): 3.7054226398468018\n",
      "Epoch: 44  Training Loss: 1.2936614955418166  Time (sec): 3.707347869873047\n",
      "Epoch: 45  Training Loss: 1.2892631724966697  Time (sec): 3.6477270126342773\n",
      "Epoch: 46  Training Loss: 1.284494979445491  Time (sec): 3.7603371143341064\n",
      "Epoch: 47  Training Loss: 1.2809502355131088  Time (sec): 3.743939161300659\n",
      "Epoch: 48  Training Loss: 1.287865069547702  Time (sec): 3.723872423171997\n",
      "Epoch: 49  Training Loss: 1.27770325121947  Time (sec): 3.7269747257232666\n",
      "Epoch: 50  Training Loss: 1.277227029519419  Time (sec): 3.766303062438965\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>valid_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>valid_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>valid_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[128, 64, 10]</td>\n",
       "      <td>[2.372253288068793, 2.0845882980288866, 1.9713...</td>\n",
       "      <td>0.545133</td>\n",
       "      <td>0.4894</td>\n",
       "      <td>0.526410</td>\n",
       "      <td>0.475292</td>\n",
       "      <td>0.545132</td>\n",
       "      <td>0.489181</td>\n",
       "      <td>0.522728</td>\n",
       "      <td>0.470682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[256, 128, 64, 10]</td>\n",
       "      <td>[2.2195624570115497, 1.9551474323062443, 1.865...</td>\n",
       "      <td>0.618289</td>\n",
       "      <td>0.5148</td>\n",
       "      <td>0.616014</td>\n",
       "      <td>0.513353</td>\n",
       "      <td>0.618221</td>\n",
       "      <td>0.514695</td>\n",
       "      <td>0.612828</td>\n",
       "      <td>0.509628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[512, 256, 128, 64, 10]</td>\n",
       "      <td>[2.219754643508514, 1.951666919253358, 1.84264...</td>\n",
       "      <td>0.642956</td>\n",
       "      <td>0.5154</td>\n",
       "      <td>0.641013</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>0.642848</td>\n",
       "      <td>0.516377</td>\n",
       "      <td>0.636344</td>\n",
       "      <td>0.510124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Hyperparameters                                               loss  \\\n",
       "0            [128, 64, 10]  [2.372253288068793, 2.0845882980288866, 1.9713...   \n",
       "1       [256, 128, 64, 10]  [2.2195624570115497, 1.9551474323062443, 1.865...   \n",
       "2  [512, 256, 128, 64, 10]  [2.219754643508514, 1.951666919253358, 1.84264...   \n",
       "\n",
       "   train_acc  valid_acc  train_precision  valid_precision  train_recall  \\\n",
       "0   0.545133     0.4894         0.526410         0.475292      0.545132   \n",
       "1   0.618289     0.5148         0.616014         0.513353      0.618221   \n",
       "2   0.642956     0.5154         0.641013         0.514520      0.642848   \n",
       "\n",
       "   valid_recall  train_f1  valid_f1  \n",
       "0      0.489181  0.522728  0.470682  \n",
       "1      0.514695  0.612828  0.509628  \n",
       "2      0.516377  0.636344  0.510124  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from algorithms import Evals\n",
    "\n",
    "# Setup activation experiment\n",
    "structure_list = [[128, 64, 10], [256, 128, 64, 10], [512, 256, 128, 64, 10]]\n",
    "\n",
    "nns, hyperparams = Evals.set_exp(structure_list, 'structure')\n",
    "eval_batch_df = Evals.run_exp(nns= nns, hyperparams= hyperparams, epochs= 50)\n",
    "eval_batch_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
