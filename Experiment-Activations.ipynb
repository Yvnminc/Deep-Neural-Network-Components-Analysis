{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Training Loss: 2.02052762805905  Time (sec): 9.923868179321289\n",
      "Epoch: 1  Training Loss: 1.9475573051170736  Time (sec): 13.074454069137573\n",
      "Epoch: 1  Training Loss: 2.0134653474888773  Time (sec): 15.64151406288147\n",
      "Epoch: 1  Training Loss: 2.001051102515774  Time (sec): 12.229875802993774\n",
      "Epoch: 1  Training Loss: 1.977503148861267  Time (sec): 9.93013596534729\n",
      "Epoch: 1  Training Loss: 2.0081811226030792  Time (sec): 16.216066122055054\n",
      "Epoch: 1  Training Loss: 1.974280055332894  Time (sec): 20.272305011749268\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>valid_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>valid_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>valid_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[relu]</td>\n",
       "      <td>[2.02052762805905]</td>\n",
       "      <td>0.377756</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.381912</td>\n",
       "      <td>0.358442</td>\n",
       "      <td>0.377892</td>\n",
       "      <td>0.358016</td>\n",
       "      <td>0.376773</td>\n",
       "      <td>0.355026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tanh]</td>\n",
       "      <td>[1.9475573051170736]</td>\n",
       "      <td>0.376422</td>\n",
       "      <td>0.3638</td>\n",
       "      <td>0.371137</td>\n",
       "      <td>0.355895</td>\n",
       "      <td>0.376526</td>\n",
       "      <td>0.362702</td>\n",
       "      <td>0.372208</td>\n",
       "      <td>0.357431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[logistic]</td>\n",
       "      <td>[2.0134653474888773]</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>0.341944</td>\n",
       "      <td>0.338339</td>\n",
       "      <td>0.343974</td>\n",
       "      <td>0.340407</td>\n",
       "      <td>0.341089</td>\n",
       "      <td>0.337359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[leaky_relu]</td>\n",
       "      <td>[2.001051102515774]</td>\n",
       "      <td>0.384156</td>\n",
       "      <td>0.3662</td>\n",
       "      <td>0.383676</td>\n",
       "      <td>0.361799</td>\n",
       "      <td>0.384249</td>\n",
       "      <td>0.364848</td>\n",
       "      <td>0.383301</td>\n",
       "      <td>0.362649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[elu]</td>\n",
       "      <td>[1.977503148861267]</td>\n",
       "      <td>0.382756</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.380239</td>\n",
       "      <td>0.361969</td>\n",
       "      <td>0.382897</td>\n",
       "      <td>0.367963</td>\n",
       "      <td>0.378999</td>\n",
       "      <td>0.362711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[gelu]</td>\n",
       "      <td>[2.0081811226030792]</td>\n",
       "      <td>0.373644</td>\n",
       "      <td>0.3604</td>\n",
       "      <td>0.372777</td>\n",
       "      <td>0.355654</td>\n",
       "      <td>0.373709</td>\n",
       "      <td>0.359812</td>\n",
       "      <td>0.370431</td>\n",
       "      <td>0.354594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[swish]</td>\n",
       "      <td>[1.974280055332894]</td>\n",
       "      <td>0.386289</td>\n",
       "      <td>0.3808</td>\n",
       "      <td>0.384911</td>\n",
       "      <td>0.378073</td>\n",
       "      <td>0.386378</td>\n",
       "      <td>0.380340</td>\n",
       "      <td>0.382774</td>\n",
       "      <td>0.376484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hyperparameters                  loss  train_acc  valid_acc  \\\n",
       "0          [relu]    [2.02052762805905]   0.377756     0.3590   \n",
       "1          [tanh]  [1.9475573051170736]   0.376422     0.3638   \n",
       "2      [logistic]  [2.0134653474888773]   0.343800     0.3418   \n",
       "3    [leaky_relu]   [2.001051102515774]   0.384156     0.3662   \n",
       "4           [elu]   [1.977503148861267]   0.382756     0.3690   \n",
       "5          [gelu]  [2.0081811226030792]   0.373644     0.3604   \n",
       "6         [swish]   [1.974280055332894]   0.386289     0.3808   \n",
       "\n",
       "   train_precision  valid_precision  train_recall  valid_recall  train_f1  \\\n",
       "0         0.381912         0.358442      0.377892      0.358016  0.376773   \n",
       "1         0.371137         0.355895      0.376526      0.362702  0.372208   \n",
       "2         0.341944         0.338339      0.343974      0.340407  0.341089   \n",
       "3         0.383676         0.361799      0.384249      0.364848  0.383301   \n",
       "4         0.380239         0.361969      0.382897      0.367963  0.378999   \n",
       "5         0.372777         0.355654      0.373709      0.359812  0.370431   \n",
       "6         0.384911         0.378073      0.386378      0.380340  0.382774   \n",
       "\n",
       "   valid_f1  \n",
       "0  0.355026  \n",
       "1  0.357431  \n",
       "2  0.337359  \n",
       "3  0.362649  \n",
       "4  0.362711  \n",
       "5  0.354594  \n",
       "6  0.376484  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from algorithms import Evals\n",
    "\n",
    "# Setup activation experiment\n",
    "activaitons = ['relu', 'tanh', 'logistic', 'leaky_relu', 'elu', 'gelu', 'swish']\n",
    "nns, hyperparams = Evals.set_act(activaitons)\n",
    "eval_batch_df = Evals.run_exp(nns= nns, hyperparams= hyperparams, epochs= 1)\n",
    "eval_batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>valid_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>valid_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>valid_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 0.001]</td>\n",
       "      <td>[2.347422747927896, 2.0398698247378975, 1.9274...</td>\n",
       "      <td>0.384422</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.380299</td>\n",
       "      <td>0.368654</td>\n",
       "      <td>0.384362</td>\n",
       "      <td>0.375762</td>\n",
       "      <td>0.379457</td>\n",
       "      <td>0.368757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[64, 0.01]</td>\n",
       "      <td>[1.909714378767329, 1.663838395213801, 1.57668...</td>\n",
       "      <td>0.538533</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>0.535047</td>\n",
       "      <td>0.495693</td>\n",
       "      <td>0.538536</td>\n",
       "      <td>0.503238</td>\n",
       "      <td>0.530227</td>\n",
       "      <td>0.494353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[64, 0.1]</td>\n",
       "      <td>[1.7159910394168398, 1.655288091550826, 1.8317...</td>\n",
       "      <td>0.295511</td>\n",
       "      <td>0.2864</td>\n",
       "      <td>0.296937</td>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.295388</td>\n",
       "      <td>0.287338</td>\n",
       "      <td>0.255951</td>\n",
       "      <td>0.249235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[128, 0.001]</td>\n",
       "      <td>[2.453707327251231, 2.1993595822872796, 2.0829...</td>\n",
       "      <td>0.330378</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.336684</td>\n",
       "      <td>0.324780</td>\n",
       "      <td>0.330298</td>\n",
       "      <td>0.321325</td>\n",
       "      <td>0.331323</td>\n",
       "      <td>0.321106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[128, 0.01]</td>\n",
       "      <td>[2.0145528386012037, 1.7353796500473972, 1.642...</td>\n",
       "      <td>0.506644</td>\n",
       "      <td>0.4712</td>\n",
       "      <td>0.501281</td>\n",
       "      <td>0.464072</td>\n",
       "      <td>0.506655</td>\n",
       "      <td>0.471504</td>\n",
       "      <td>0.499820</td>\n",
       "      <td>0.464135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[128, 0.1]</td>\n",
       "      <td>[1.7205033968758368, 1.5663508174462686, 1.560...</td>\n",
       "      <td>0.446467</td>\n",
       "      <td>0.4278</td>\n",
       "      <td>0.455477</td>\n",
       "      <td>0.433481</td>\n",
       "      <td>0.446485</td>\n",
       "      <td>0.428230</td>\n",
       "      <td>0.443393</td>\n",
       "      <td>0.424625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[256, 0.001]</td>\n",
       "      <td>[2.5758271271477784, 2.37177000194671, 2.23792...</td>\n",
       "      <td>0.273444</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.274479</td>\n",
       "      <td>0.268095</td>\n",
       "      <td>0.273401</td>\n",
       "      <td>0.268973</td>\n",
       "      <td>0.271799</td>\n",
       "      <td>0.266748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[256, 0.01]</td>\n",
       "      <td>[2.211865496709712, 1.8624782715029875, 1.7545...</td>\n",
       "      <td>0.447911</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.444611</td>\n",
       "      <td>0.415172</td>\n",
       "      <td>0.447889</td>\n",
       "      <td>0.420618</td>\n",
       "      <td>0.445047</td>\n",
       "      <td>0.416036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[256, 0.1]</td>\n",
       "      <td>[1.7870716143649887, 1.5410940505545827, 1.488...</td>\n",
       "      <td>0.521711</td>\n",
       "      <td>0.4972</td>\n",
       "      <td>0.518743</td>\n",
       "      <td>0.494159</td>\n",
       "      <td>0.521594</td>\n",
       "      <td>0.498309</td>\n",
       "      <td>0.514234</td>\n",
       "      <td>0.490754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hyperparameters                                               loss  \\\n",
       "0     [64, 0.001]  [2.347422747927896, 2.0398698247378975, 1.9274...   \n",
       "1      [64, 0.01]  [1.909714378767329, 1.663838395213801, 1.57668...   \n",
       "2       [64, 0.1]  [1.7159910394168398, 1.655288091550826, 1.8317...   \n",
       "3    [128, 0.001]  [2.453707327251231, 2.1993595822872796, 2.0829...   \n",
       "4     [128, 0.01]  [2.0145528386012037, 1.7353796500473972, 1.642...   \n",
       "5      [128, 0.1]  [1.7205033968758368, 1.5663508174462686, 1.560...   \n",
       "6    [256, 0.001]  [2.5758271271477784, 2.37177000194671, 2.23792...   \n",
       "7     [256, 0.01]  [2.211865496709712, 1.8624782715029875, 1.7545...   \n",
       "8      [256, 0.1]  [1.7870716143649887, 1.5410940505545827, 1.488...   \n",
       "\n",
       "   train_acc  valid_acc  train_precision  valid_precision  train_recall  \\\n",
       "0   0.384422     0.3750         0.380299         0.368654      0.384362   \n",
       "1   0.538533     0.5028         0.535047         0.495693      0.538536   \n",
       "2   0.295511     0.2864         0.296937         0.285146      0.295388   \n",
       "3   0.330378     0.3208         0.336684         0.324780      0.330298   \n",
       "4   0.506644     0.4712         0.501281         0.464072      0.506655   \n",
       "5   0.446467     0.4278         0.455477         0.433481      0.446485   \n",
       "6   0.273444     0.2686         0.274479         0.268095      0.273401   \n",
       "7   0.447911     0.4200         0.444611         0.415172      0.447889   \n",
       "8   0.521711     0.4972         0.518743         0.494159      0.521594   \n",
       "\n",
       "   valid_recall  train_f1  valid_f1  \n",
       "0      0.375762  0.379457  0.368757  \n",
       "1      0.503238  0.530227  0.494353  \n",
       "2      0.287338  0.255951  0.249235  \n",
       "3      0.321325  0.331323  0.321106  \n",
       "4      0.471504  0.499820  0.464135  \n",
       "5      0.428230  0.443393  0.424625  \n",
       "6      0.268973  0.271799  0.266748  \n",
       "7      0.420618  0.445047  0.416036  \n",
       "8      0.498309  0.514234  0.490754  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.DataFrame(eval_dict)\n",
    "eval_df\n",
    "\n",
    "hyperparameters = []\n",
    "for i in batch_size:\n",
    "    for j in learning_rate:\n",
    "        hyperparameters.append([i, j])\n",
    "\n",
    "# Insert the hyperparameters into the dataframe with first column\n",
    "eval_df.insert(0, \"Hyperparameters\", hyperparameters, True)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 128)\n",
      "(45000, 10)\n",
      "(5000, 128)\n",
      "(5000, 10)\n",
      "(10000, 128)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from algorithms import *\n",
    "from algorithms.MlpV2 import *\n",
    "data = Data()\n",
    "data.print_shapes()\n",
    "X_train = data.train_data\n",
    "y_train = data.train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data.test_data\n",
    "y_test = data.test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = [\"relu\", \"logistic\", \"tanh\", \"leaky_relu\", \"swish\", \"gelu\", \"elu\"]\n",
    "lr = 0.1\n",
    "loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_precision = []\n",
    "test_precision = []\n",
    "train_recall = []\n",
    "test_recall = []\n",
    "train_f1 = []\n",
    "test_f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Training Loss: 1.8026439958840028  Time (sec): 13.220166206359863\n",
      "Epoch: 2  Training Loss: 1.5471248041676842  Time (sec): 16.270312786102295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m nn\u001b[39m.\u001b[39madd_layer(\u001b[39m128\u001b[39m,\u001b[39m64\u001b[39m,activation,\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m nn\u001b[39m.\u001b[39madd_layer(\u001b[39m64\u001b[39m,\u001b[39m10\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yann/Desktop/COMP5329_A1/Experiment-Activations.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m acc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mevaluate(X_train, y_train)\n",
      "File \u001b[0;32m~/Desktop/COMP5329_A1/algorithms/MlpV2.py:171\u001b[0m, in \u001b[0;36mMlpV2.fit\u001b[0;34m(self, X, y, epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m    170\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m, size \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[1;32m    172\u001b[0m     \u001b[39m#get mean loss of all batch losses\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[39m#print(self.batch.loss)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     mean_loss_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch\u001b[39m.\u001b[39mgetLoss())\n",
      "File \u001b[0;32m~/Desktop/COMP5329_A1/algorithms/MiniBatchTrainingV2.py:76\u001b[0m, in \u001b[0;36mMiniBatchV2.fit\u001b[0;34m(self, model, size)\u001b[0m\n\u001b[1;32m     72\u001b[0m mini_X \u001b[39m=\u001b[39m shuff_X[start:end,:]\n\u001b[1;32m     73\u001b[0m mini_Y \u001b[39m=\u001b[39m shuff_Y[start:end,:]\n\u001b[0;32m---> 76\u001b[0m mini_Y_hat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(mini_X, mode \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mregularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mappend( model\u001b[39m.\u001b[39mcriterion_cross_entropy(mini_Y, mini_Y_hat)[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m model\u001b[39m.\u001b[39mregularizer\u001b[39m.\u001b[39mget_reg_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm))\n",
      "File \u001b[0;32m~/Desktop/COMP5329_A1/algorithms/MlpV2.py:104\u001b[0m, in \u001b[0;36mMlpV2.forward\u001b[0;34m(self, input, mode)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer\u001b[39m.\u001b[39mreset()   \n\u001b[1;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m    103\u001b[0m     \u001b[39m#print(input.shape)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     output \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m, regularizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregularizer, train_mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    105\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m output\n\u001b[1;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/COMP5329_A1/algorithms/HiddenLayer.py:94\u001b[0m, in \u001b[0;36mHiddenLayer.forward\u001b[0;34m(self, input, train_mode, regularizer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m#print(\"n_node\", self.n_in)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m#print(\"input\",input.shape)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m---> 94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[1;32m     95\u001b[0m \u001b[39m#print(\"z\",self.z.shape)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[39m#print(\"w\",self.W.shape)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# batch normalization\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchNormalizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for activation in activations:\n",
    "    nn = MlpV2(learning_rate = lr, batch_size = 256)\n",
    "    nn.set_optimiser(opt_type='Momentum', params = [0.9])\n",
    "    nn.set_batchNormalizer()\n",
    "    nn.add_layer(128,512,activation,1)\n",
    "    nn.add_layer(512,256,activation,1)\n",
    "    nn.add_layer(256,128,activation,1)\n",
    "    nn.add_layer(128,64,activation,1)\n",
    "    nn.add_layer(64,10,\"softmax\",1)\n",
    "\n",
    "    train_loss = nn.fit(X_train, y_train, epochs=50)\n",
    "    loss.append(train_loss)\n",
    "\n",
    "    acc = nn.evaluate(X_train, y_train)\n",
    "    train_acc.append(acc)\n",
    "\n",
    "    t_acc = nn.evaluate(X_test, y_test)\n",
    "    test_acc.append(t_acc)\n",
    "    \n",
    "    y_pred_train = np.argmax(nn.predict(X_train), axis=1)\n",
    "    y_train_transformed = np.argmax(y_train, axis=1)\n",
    "    y_pred_test = np.argmax(nn.predict(X_test), axis=1)\n",
    "    y_test_transformed = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    precision = precision_score(y_train_transformed, y_pred_train, average='macro')\n",
    "    recall = recall_score(y_train_transformed, y_pred_train, average='macro')\n",
    "    f1 = f1_score(y_train_transformed, y_pred_train, average='macro')\n",
    "    \n",
    "    train_precision.append(precision)\n",
    "    train_recall.append(recall)\n",
    "    train_f1.append(f1)\n",
    "    \n",
    "    t_precision = precision_score(y_test_transformed, y_pred_test, average='macro')\n",
    "    t_recall = recall_score(y_test_transformed, y_pred_test, average='macro')\n",
    "    t_f1 = f1_score(y_test_transformed, y_pred_test, average='macro')\n",
    "    \n",
    "    test_precision.append(t_precision)\n",
    "    test_recall.append(t_recall)\n",
    "    test_f1.append(t_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
